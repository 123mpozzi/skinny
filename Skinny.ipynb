{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wWuJLG8FFDI"
   },
   "source": [
    "Check Python, Tensorflow, CUDA versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQtNt5byE3W5",
    "outputId": "c8ab224c-0c45-4893-c315-9c9903908db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n",
      "2021-05-23 19:18:24.481383: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2.4.1\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!python3 -c 'import tensorflow as tf; print(tf.__version__)'\n",
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuumSeLG01am"
   },
   "source": [
    "DATASET Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRm1MbAk01q3",
    "outputId": "cab9d654-d167-4167-db46-b2390c7309a6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlbfIVMfz2xD"
   },
   "outputs": [],
   "source": [
    "# import a manually uploaded Schmugge subsplit-augmented(eg: dark) dataset\n",
    "import_schmugge_aug = False\n",
    "\n",
    "if import_schmugge_aug:\n",
    "    !rm -rf dataset/Schmugge\n",
    "    !unzip drive/MyDrive/schm/Schmugge.zip -d dataset # schmugge dark aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qp3GYPvMCciv"
   },
   "source": [
    "Extract the dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXWRZ3UjdOQh",
    "outputId": "40e9fd6f-27ce-403d-8ee0-16f84ace819a"
   },
   "outputs": [],
   "source": [
    "!rm -rf dataset\n",
    "\n",
    "# full body\n",
    "!unzip drive/MyDrive/datasets/fullbody/ECU.zip -d dataset\n",
    "#!unzip drive/MyDrive/datasets/fullbody/VDM.zip -d dataset\n",
    "#!unzip drive/MyDrive/datasets/fullbody/Uchile.zip -d dataset\n",
    "#!unzip drive/MyDrive/datasets/fullbody/Pratheepan.zip -d dataset\n",
    "\n",
    "# hand\n",
    "!unzip drive/MyDrive/datasets/hand/HGR_small.zip -d dataset\n",
    "\n",
    "# face\n",
    "!unzip drive/MyDrive/datasets/face/Schmugge.zip -d dataset\n",
    "\n",
    "# abdomen\n",
    "#!unzip drive/MyDrive/datasets/abdomen/abd-skin.zip -d dataset\n",
    "\n",
    "# import-db.json files\n",
    "!unzip drive/MyDrive/datasets/imports/dataset_imports.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Khw4fujbYapP"
   },
   "source": [
    "Dataset Organization Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RpMl_DyW82c"
   },
   "outputs": [],
   "source": [
    "import os, re, sys, json\n",
    "import cv2 # to load, save, process images\n",
    "import imghdr # to check if a file is an image\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from math import floor\n",
    "\n",
    "# remember that Pratheepan dataset has one file with comma in the filename\n",
    "csv_sep = '?'\n",
    "\n",
    "\n",
    "def get_training_and_testing_sets(file_list: list, split: float = 0.7):\n",
    "    print(file_list)\n",
    "    split_index = floor(len(file_list) * split)\n",
    "    training = file_list[:split_index]\n",
    "    testing = file_list[split_index:]\n",
    "    return training, testing\n",
    "\n",
    "# Get the variable part of a filename into a dataset\n",
    "def get_variable_filename(filename: str, format: str) -> str:\n",
    "    if format == '':\n",
    "        return filename\n",
    "\n",
    "    # re.fullmatch(r'^img(.*)$', 'imgED (1)').group(1)\n",
    "    # re.fullmatch(r'^(.*)-m$', 'att-massu.jpg-m').group(1)\n",
    "    match =  re.fullmatch('^{}$'.format(format), filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        #print('Cannot match {} with pattern {}'.format(filename, format))\n",
    "        return None\n",
    "\n",
    "# args eg: datasets/ECU/skin_masks datasets/ECU/original_images datasets/ecu/\n",
    "# note: NonDefined, TRain, TEst, VAlidation\n",
    "def analyze_dataset(gt: str, ori: str, root_dir: str, note: str = 'nd',\n",
    "                    gt_filename_format: str = '', ori_filename_format: str = '',\n",
    "                    gt_ext: str = '', ori_ext: str = '') -> None:\n",
    "    out_analysis_filename = 'data.csv'\n",
    "\n",
    "    out_analysis = os.path.join(root_dir, out_analysis_filename)\n",
    "    analyze_content(gt, ori, out_analysis, note = note,\n",
    "                    gt_filename_format = gt_filename_format,\n",
    "                    ori_filename_format = ori_filename_format,\n",
    "                    gt_ext = gt_ext, ori_ext = ori_ext)\n",
    "\n",
    "# creates a file with lines like: origina_image1.jpg, skin_mask1.png, tr\n",
    "def analyze_content(gt: str, ori: str, outfile: str, note: str = 'nd',\n",
    "                    gt_filename_format: str = '', ori_filename_format: str = '',\n",
    "                    gt_ext: str = '', ori_ext: str = '') -> None:\n",
    "    # images found\n",
    "    i = 0\n",
    "\n",
    "    # append to data file\n",
    "    with open(outfile, 'a') as out:\n",
    "\n",
    "        for gt_file in os.listdir(gt):\n",
    "            gt_path = os.path.join(gt, gt_file)\n",
    "\n",
    "            # controlla se e' un'immagine (per evitare problemi con files come thumbs.db)\n",
    "            if not os.path.isdir(gt_path) and imghdr.what(gt_path) != None:\n",
    "                matched = False\n",
    "                gt_name, gt_e = os.path.splitext(gt_file)\n",
    "                gt_identifier = get_variable_filename(gt_name, gt_filename_format)\n",
    "\n",
    "                if gt_identifier == None:\n",
    "                    continue\n",
    "\n",
    "                if gt_ext and gt_e != '.' + gt_ext:\n",
    "                    continue\n",
    "                \n",
    "                for ori_file in os.listdir(ori):\n",
    "                    ori_path = os.path.join(ori, ori_file)\n",
    "                    ori_name, ori_e = os.path.splitext(ori_file)\n",
    "                    ori_identifier = get_variable_filename(ori_name, ori_filename_format)\n",
    "                    \n",
    "                    if ori_identifier == None:\n",
    "                        continue\n",
    "\n",
    "                    if ori_ext and ori_e != '.' + ori_ext:\n",
    "                        continue\n",
    "                    \n",
    "                    # try to find a match (original image - gt)\n",
    "                    if gt_identifier == ori_identifier:\n",
    "                        out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}\\n\")\n",
    "                        i += 1\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                if not matched:\n",
    "                    print(f'No matches found for {gt_identifier}')\n",
    "            else:\n",
    "                print(f'File {gt_path} is not an image')\n",
    "        \n",
    "        print(f\"Found {i} images\")\n",
    "\n",
    "# Does a simple processing on all dataset images based on a few operations.\n",
    "# Used to make ground truth masks uniform across the datasets.\n",
    "# \n",
    "# (JSON) \"processpipe\" : \"png,skin=255_255_255,invert\"\n",
    "#        \"processout\" : \"out/process/folder\" \n",
    "# skin=... vuol dire che la regola per binarizzare è: tutto quello che non è skin va a nero, skin a bianco\n",
    "# bg=... viceversa\n",
    "#    (quindi skin e bg fanno anche binarizzazione!)\n",
    "# *il processing viene fatto nell'ordine scritto!!!\n",
    "def process_images(data_dir: str, process_pipeline: str, out_dir = '',\n",
    "                   im_filename_format: str = '', im_ext: str = '') -> str:\n",
    "\n",
    "    # loop mask files\n",
    "    for im_basename in os.listdir(data_dir):\n",
    "        im_path = os.path.join(data_dir, im_basename)\n",
    "        im_filename, im_e = os.path.splitext(im_basename)\n",
    "\n",
    "        # controlla se e' un'immagine (per evitare problemi con files come thumbs.db)\n",
    "        if not os.path.isdir(im_path) and imghdr.what(im_path) != None:\n",
    "            if out_dir == '':\n",
    "                out_dir = os.path.join(data_dir, 'processed')\n",
    "\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            im_identifier = get_variable_filename(im_filename, im_filename_format)\n",
    "            if im_identifier == None:\n",
    "                continue\n",
    "            \n",
    "            if im_ext and im_e != '.' + im_ext:\n",
    "                continue\n",
    "\n",
    "            # load image\n",
    "            im = cv2.imread(im_path)\n",
    "\n",
    "            # prepare path for out image\n",
    "            im_path = os.path.join(out_dir, im_basename)\n",
    "\n",
    "            for operation in process_pipeline.split(','):\n",
    "                # binarize. Rule: what isn't skin is black\n",
    "                if operation.startswith('skin'):\n",
    "                    # inspired from https://stackoverflow.com/a/53989391\n",
    "                    bgr_data = operation.split('=')[1]\n",
    "                    bgr_chs = bgr_data.split('_')\n",
    "                    b = int(bgr_chs[0])\n",
    "                    g = int(bgr_chs[1])\n",
    "                    r = int(bgr_chs[2])\n",
    "                    lower_val = (b, g, r)\n",
    "                    upper_val = lower_val\n",
    "                    # Threshold the image to get only selected colors\n",
    "                    # what isn't skin is black\n",
    "                    mask = cv2.inRange(im, lower_val, upper_val)\n",
    "                    im = mask\n",
    "                # binarize. Rule: what isn't bg is white\n",
    "                elif operation.startswith('bg'):\n",
    "                    bgr_data = operation.split('=')[1]\n",
    "                    bgr_chs = bgr_data.split('_')\n",
    "                    b = int(bgr_chs[0])\n",
    "                    g = int(bgr_chs[1])\n",
    "                    r = int(bgr_chs[2])\n",
    "                    lower_val = (b, g, r)\n",
    "                    upper_val = lower_val\n",
    "                    # Threshold the image to get only selected colors\n",
    "                    mask = cv2.inRange(im, lower_val, upper_val)\n",
    "                    #cv2_imshow(mask) #debug\n",
    "                    # what isn't bg is white\n",
    "                    sk = cv2.bitwise_not(mask)\n",
    "                    im = sk\n",
    "                # invert image\n",
    "                elif operation == 'invert':\n",
    "                    im = cv2.bitwise_not(im)\n",
    "                # convert to png\n",
    "                elif operation == 'png':\n",
    "                    im_path = os.path.join(out_dir, im_filename + '.png')\n",
    "                # reload image\n",
    "                elif operation == 'reload':\n",
    "                    im = cv2.imread(im_path)\n",
    "                else:\n",
    "                    print(f'Image processing operation unknown: {operation}')\n",
    "\n",
    "            # save processing \n",
    "            cv2.imwrite(im_path, im)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "# update the csv by adding a split from a different-format file (1 column split)\n",
    "def import_split(csv_file: str, single_col_file: str, outfile: str,\n",
    "                 note: str, gtf = '', orif = '', inf = '') -> None:\n",
    "    # read csv lines\n",
    "    file3c = open(csv_file)\n",
    "    triples = file3c.read().splitlines()\n",
    "    file3c.close()\n",
    "    \n",
    "    # read single column file lines\n",
    "    file1c = open(single_col_file)\n",
    "    singles = file1c.read().splitlines()\n",
    "    file1c.close()\n",
    "\n",
    "    # create the new split file as csv two columns\n",
    "    with open(os.path.join(outfile), 'w') as out:\n",
    "        i = 0\n",
    "\n",
    "        for entry in triples: # oriname.ext, gtname.ext, te/tr/va\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            note_old = entry.split(csv_sep)[2]\n",
    "            ori_name, ori_ext = os.path.splitext(os.path.basename(ori_path))\n",
    "            gt_name, gt_ext = os.path.splitext(os.path.basename(gt_path))\n",
    "\n",
    "            ori_identifier = get_variable_filename(ori_name, orif)\n",
    "            gt_identifier = get_variable_filename(gt_name, gtf)\n",
    "\n",
    "            for line in singles: # imgname\n",
    "                line_name, line_ext = os.path.splitext(line)\n",
    "                in_identifier = get_variable_filename(line_name, inf)\n",
    "\n",
    "                if ori_identifier == in_identifier or gt_identifier == in_identifier:\n",
    "                    note_old = note\n",
    "                    i += 1\n",
    "                    print(f'Match found: {ori_identifier}\\|{gt_identifier} - {in_identifier}')\n",
    "                    break # match found\n",
    "                \n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note_old}\\n\")\n",
    "        \n",
    "        print(f'''Converted {i}/{len(singles)} lines.\\n\n",
    "        Source file: {single_col_file}\\n\n",
    "        Target file: {outfile}''')\n",
    "\n",
    "# update the notes in csv by adding a split from a partial file with the same format\n",
    "def update_split(csv_file: str, partial_file: str, outfile: str, newnote: str) -> None:\n",
    "    # read csv lines\n",
    "    file3c = open(csv_file)\n",
    "    triples = file3c.read().splitlines()\n",
    "    file3c.close()\n",
    "    \n",
    "    # read partial file lines\n",
    "    file1c = open(partial_file)\n",
    "    partials = file1c.read().splitlines()\n",
    "    file1c.close()\n",
    "\n",
    "    # create the new split file as csv two columns\n",
    "    with open(os.path.join(outfile), 'w') as out:\n",
    "        i = 0\n",
    "\n",
    "        for entry in triples: # oriname.ext, gtname.ext\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            note_old = entry.split(csv_sep)[2]\n",
    "\n",
    "            skintone = ''\n",
    "            if len(entry.split(csv_sep)) == 4:\n",
    "                skintone = csv_sep + entry.split(csv_sep)[3]\n",
    "\n",
    "\n",
    "            for line in partials: # imgname\n",
    "                ori_path_part = line.split(csv_sep)[0]\n",
    "\n",
    "                if ori_path == ori_path_part:\n",
    "                    note_old = newnote\n",
    "                    i += 1\n",
    "                    print(f'Match found: {ori_path}')\n",
    "                    break # match found\n",
    "                \n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note_old}{skintone}\\n\")\n",
    "        \n",
    "        print(f'''Updated {i}/{len(partials)} lines.\\n\n",
    "        Source file: {partial_file}\\n\n",
    "        Target file: {outfile}''')\n",
    "\n",
    "# import dataset and generate metadata\n",
    "def import_dataset(import_json: str) -> None:\n",
    "    if os.path.exists(import_json):\n",
    "        with open(import_json, 'r') as stream:\n",
    "            data = json.load(stream)\n",
    "\n",
    "            # load JSON values\n",
    "            gt = data['gt']\n",
    "            ori = data['ori']\n",
    "            root = data['root']\n",
    "            note = data['note']\n",
    "            gt_format = data['gtf']\n",
    "            ori_format = data['orif']\n",
    "            gt_ext = data['gtext']\n",
    "            ori_ext = data['oriext']\n",
    "            ori_process = data['oriprocess']\n",
    "            ori_process_out = data['oriprocessout']\n",
    "            gt_process = data['gtprocess']\n",
    "            gt_process_out = data['gtprocessout']\n",
    "            \n",
    "            # check if processing is required\n",
    "            if ori_process:\n",
    "                ori = process_images(ori, ori_process, ori_process_out,\n",
    "                                     ori_format, ori_ext)\n",
    "                # update the file extension in the images are being converted\n",
    "                if 'png' in ori_process:\n",
    "                    ori_ext = 'png'\n",
    "            \n",
    "            if gt_process:\n",
    "                gt = process_images(gt, gt_process, gt_process_out,\n",
    "                                     gt_format, gt_ext)\n",
    "                if 'png' in gt_process:\n",
    "                    gt_ext = 'png'\n",
    "            \n",
    "            # Non-Defined as default note\n",
    "            if not note:\n",
    "                note = 'nd'\n",
    "            \n",
    "            # analyze the dataset and create the csv files\n",
    "            analyze_dataset(gt, ori, root,\n",
    "                            note, gt_format, ori_format,\n",
    "                            gt_ext, ori_ext)\n",
    "    else:\n",
    "        print(\"JSON import file does not exist!\")\n",
    "\n",
    "def get_timestamp() -> str:\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# from schmugge custom config (.config.SkinImManager) to a list of dict structure\n",
    "def read_schmugge(skin_im_manager_path: str, images_dir: str) -> list: # also prepare the csv\n",
    "    sch = []\n",
    "    \n",
    "    # images with gt errors, aa69 is also duplicated in the config file\n",
    "    blacklist = ['aa50.gt.d3.pgm', 'aa69.gt.d3.pgm', 'dd71.gt.d3.pgm', 'hh54.gt.d3.pgm']\n",
    "\n",
    "    with open(skin_im_manager_path) as f:\n",
    "        start = 0\n",
    "        i = 0\n",
    "        tmp = {}\n",
    "        for line in f:\n",
    "            blacklisted = False\n",
    "\n",
    "            if start < 2: # skip first 2 lines\n",
    "                start += 1\n",
    "                continue\n",
    "            \n",
    "            #print(f'{line}\\t{i}') # debug\n",
    "            if line: # line not empty\n",
    "                line = line.rstrip() # remove End Of Line (\\n)\n",
    "\n",
    "                if i == 2: # skin tone type\n",
    "                    skin_type = int(line)\n",
    "                    if skin_type == 0:\n",
    "                        tmp['skintone'] = 'light'\n",
    "                    elif skin_type == 1:\n",
    "                        tmp['skintone'] = 'medium'\n",
    "                    elif skin_type == 2:\n",
    "                        tmp['skintone'] = 'dark'\n",
    "                    else:\n",
    "                        tmp['skintone'] = 'nd'\n",
    "                elif i == 3: # db type\n",
    "                    tmp['db'] = line\n",
    "                elif i == 8: # ori\n",
    "                    tmp['ori'] = os.path.join(images_dir, line)\n",
    "                elif i == 9: # gt\n",
    "                    tmp['gt'] = os.path.join(images_dir, line)\n",
    "                    if line in blacklist:\n",
    "                        blacklisted = True\n",
    "                \n",
    "\n",
    "                # update image counter\n",
    "                i += 1\n",
    "                if i == 10: # 10 lines read, prepare for next image data\n",
    "                    if not blacklisted:\n",
    "                        sch.append(tmp)\n",
    "                    tmp = {}\n",
    "                    i = 0\n",
    "    \n",
    "    print(f'Schmugge custom config read correctly, found {len(sch)} images')\n",
    "\n",
    "    return sch\n",
    "\n",
    "# from schmugge list of dicts structure to csv file and processed images\n",
    "def process_schmugge(sch: list, outfile: str, train = 70, test = 15, val = 15, ori_out_dir = 'new_ori', gt_out_dir = 'new_gt'):\n",
    "    # prepare new ori and gt dirs\n",
    "    os.makedirs(ori_out_dir, exist_ok=True)\n",
    "    os.makedirs(gt_out_dir, exist_ok=True)\n",
    "\n",
    "    with open(outfile, 'w') as out:\n",
    "        shuffle(sch) # randomize\n",
    "\n",
    "        # 70% train, 15% val, 15% test\n",
    "        train_files, test_files = get_training_and_testing_sets(sch)\n",
    "        test_files, val_files = get_training_and_testing_sets(test_files, split=.5)\n",
    "\n",
    "        for entry in sch:\n",
    "            db = int(entry['db'])\n",
    "            ori_path = entry['ori']\n",
    "            gt_path = entry['gt']\n",
    "            \n",
    "\n",
    "            ori_basename = os.path.basename(ori_path)\n",
    "            gt_basename = os.path.basename(gt_path)\n",
    "            ori_filename, ori_e = os.path.splitext(ori_basename)\n",
    "            gt_filename, gt_e = os.path.splitext(gt_basename)\n",
    "\n",
    "            # process images\n",
    "            # load images\n",
    "            ori_im = cv2.imread(ori_path)\n",
    "            gt_im = cv2.imread(gt_path)\n",
    "            # png\n",
    "            ori_out = os.path.join(ori_out_dir, ori_filename + '.png')\n",
    "            gt_out = os.path.join(gt_out_dir, gt_filename + '.png')\n",
    "            # binarize gt: whatever isn't background, is skin\n",
    "            if db == 4 or db == 3: # Uchile/UW: white background\n",
    "                b = 255\n",
    "                g = 255\n",
    "                r = 255\n",
    "                lower_val = (b, g, r)\n",
    "                upper_val = lower_val\n",
    "                # Threshold the image to get only selected colors\n",
    "                mask = cv2.inRange(gt_im, lower_val, upper_val)\n",
    "                #cv2_imshow(mask) #debug\n",
    "                # what isn't bg is white\n",
    "                sk = cv2.bitwise_not(mask)\n",
    "                gt_im = sk\n",
    "            else: # background = 180,180,180\n",
    "                b = 180\n",
    "                g = 180\n",
    "                r = 180\n",
    "                lower_val = (b, g, r)\n",
    "                upper_val = lower_val\n",
    "                # Threshold the image to get only selected colors\n",
    "                mask = cv2.inRange(gt_im, lower_val, upper_val)\n",
    "                #cv2_imshow(mask) #debug\n",
    "                # what isn't bg is white\n",
    "                sk = cv2.bitwise_not(mask)\n",
    "                gt_im = sk\n",
    "            # save processing \n",
    "            cv2.imwrite(ori_out, ori_im)\n",
    "            cv2.imwrite(gt_out, gt_im)\n",
    "\n",
    "            skintone = entry['skintone']\n",
    "            note = 'te'\n",
    "            if entry in train_files:\n",
    "                note = 'tr'\n",
    "            elif entry in val_files:\n",
    "                note = 'va'\n",
    "            \n",
    "            out.write(f\"{ori_out}{csv_sep}{gt_out}{csv_sep}{note}{csv_sep}{skintone}\\n\")\n",
    "\n",
    "# write all csv line note attributes as the given argument\n",
    "def csv_full_test(csv_file: str, note = 'nd'):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # rewrite csv file\n",
    "    with open(csv_file, 'w') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            #note = 'nd'\n",
    "\n",
    "            # check if there is also a 4th parameter in the line (Schmugge skintones)\n",
    "            skintone = ''\n",
    "            if len(entry.split(csv_sep)) == 4:\n",
    "                skintone = csv_sep + entry.split(csv_sep)[3]\n",
    "\n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{skintone}\\n\")\n",
    "\n",
    "# write all csv line note attributes as the given argument\n",
    "def csv_count_test(csv_file: str, count: int, note = 'nd'):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # rewrite csv file\n",
    "    i = 0\n",
    "    with open(csv_file, 'w') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            #note = 'nd'\n",
    "\n",
    "            if i < count:\n",
    "                note = 'te'\n",
    "                i += 1\n",
    "            else:\n",
    "                note = 'tr'\n",
    "\n",
    "            # check if there is also a 4th parameter in the line (Schmugge skintones)\n",
    "            skintone = ''\n",
    "            if len(entry.split(csv_sep)) == 4:\n",
    "                skintone = csv_sep + entry.split(csv_sep)[3]\n",
    "\n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{skintone}\\n\")\n",
    "\n",
    "# all the items with note 'te' become 'tr', all the items with note != 'te', become 'te'\n",
    "def csv_not_test(csv_file: str):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # rewrite csv file\n",
    "    with open(csv_file, 'w') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            nt = entry.split(csv_sep)[2]\n",
    "\n",
    "            if nt == 'te':\n",
    "                note = 'tr'\n",
    "            else:\n",
    "                note = 'te'\n",
    "\n",
    "            # check if there is also a 4th parameter in the line (Schmugge skintones)\n",
    "            skintone = ''\n",
    "            if len(entry.split(csv_sep)) == 4:\n",
    "                skintone = csv_sep + entry.split(csv_sep)[3]\n",
    "\n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{skintone}\\n\")\n",
    "\n",
    "# Updates the csv file by modyfing the notes of lines of the given skintone\n",
    "# csv must have 4 cols!\n",
    "# skintone may be: 'dark', 'light', 'medium'\n",
    "def csv_skintone_filter(csv_file: str, skintone: str, mode = 'train', val_percent = .15, test_percent = .15):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # randomize\n",
    "    shuffle(file3c)\n",
    "\n",
    "    # calculate splits length\n",
    "    totalsk = csv_skintone_count(csv_file, skintone) # total items to train/val/test on\n",
    "    totalva = round(totalsk * val_percent)\n",
    "    totalte = round(totalsk * test_percent)\n",
    "    #totaltr = totalsk - totalva\n",
    "    jva = 0\n",
    "    jte = 0\n",
    "    #jtr = 0\n",
    "\n",
    "\n",
    "    # rewrite csv file\n",
    "    with open(csv_file, 'w') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "\n",
    "            skint = entry.split(csv_sep)[3]\n",
    "\n",
    "            if skint != skintone: # should not be filtered\n",
    "                note = 'nd'\n",
    "                \n",
    "                out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{csv_sep}{skint}\\n\")\n",
    "            else: # should be in the filter\n",
    "                if mode == 'train': # if it is a training filter\n",
    "                    if jva < totalva: # there are still places left to be in validation set\n",
    "                        note = 'va'\n",
    "                        jva += 1\n",
    "                    elif jte < totalte: # there are still places left to be in test set\n",
    "                        note = 'te'\n",
    "                        jte += 1\n",
    "                    else: # no more validation places to sit in, go in train set\n",
    "                        note = 'tr'\n",
    "                else: # if it is a testing filter, just place them all in test set\n",
    "                    note = 'te'\n",
    "                \n",
    "                out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{csv_sep}{skintone}\\n\")\n",
    "\n",
    "# Prints the total amount of items of the given skintone\n",
    "def csv_skintone_count(csv_file: str, skintone: str):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    j = 0\n",
    "    # read csv file\n",
    "    with open(csv_file, 'r') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            note = entry.split(csv_sep)[2]\n",
    "            skint = entry.split(csv_sep)[3]\n",
    "\n",
    "            if skint == skintone:\n",
    "                j += 1\n",
    "                print(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{csv_sep}{skint}\")\n",
    "    \n",
    "    print(f\"Found {j} items of type {skintone}\")\n",
    "    return j\n",
    "\n",
    "# Prints the total amount of items of the given mode('train'(includes validation), 'test')\n",
    "def csv_note_count(csv_file: str, mode: str):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    j = 0\n",
    "    # read csv file\n",
    "    with open(csv_file, 'r') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            nt = entry.split(csv_sep)[2]\n",
    "            skint = entry.split(csv_sep)[3]\n",
    "\n",
    "            notes = []\n",
    "            if mode == 'train':\n",
    "                notes.append(\"tr\")\n",
    "                notes.append(\"va\")\n",
    "            else:\n",
    "                notes.append(\"te\")\n",
    "\n",
    "            if nt in notes:\n",
    "                j += 1\n",
    "                print(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{nt}{csv_sep}{skint}\")\n",
    "    \n",
    "    print(f\"Found {j} items of type {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIxL3PDs1l3q"
   },
   "outputs": [],
   "source": [
    "# assumes the Schmugge dataset data.csv file is in dataset/Schmugge folder\n",
    "# mode can either be 'train' or 'test'\n",
    "def gen_sch_by_skintone(skintone: str, mode: str):\n",
    "    sch_csv = 'dataset/Schmugge/data.csv'\n",
    "\n",
    "    # re-import Schmugge\n",
    "    schm = read_schmugge('dataset/Schmugge/data/.config.SkinImManager', 'dataset/Schmugge/data/data')\n",
    "    process_schmugge(schm, sch_csv, ori_out_dir='dataset/Schmugge/newdata/ori', gt_out_dir='dataset/Schmugge/newdata/gt')\n",
    "\n",
    "    csv_skintone_filter(sch_csv, skintone, mode = mode)\n",
    "    csv_skintone_count(sch_csv, skintone)\n",
    "    #csv_note_count(filter_by_skintone_csv, filter_mode)\n",
    "\n",
    "# gen_sch_by_skintone('dark', 'train')\n",
    "# gen_sch_by_skintone('light', 'train')\n",
    "# gen_sch_by_skintone('medium', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUJHgu5IJJxv",
    "outputId": "73e58aa2-2887-4988-fbe6-ac8ccbab7885"
   },
   "outputs": [],
   "source": [
    "# generate datasets metadata\n",
    "\n",
    "# simple datasets\n",
    "import_dataset(\"dataset/import_ecu.json\")\n",
    "#import_dataset(\"dataset/import_uchile.json\")\n",
    "\n",
    "# hgr is composed of 3 sub datasets\n",
    "import_dataset(\"dataset/import_hgr1.json\")\n",
    "import_dataset(\"dataset/import_hgr2a.json\")\n",
    "import_dataset(\"dataset/import_hgr2b.json\")\n",
    "\n",
    "# pratheepan is composed of 2 sub datasets\n",
    "#import_dataset(\"dataset/import_pratheepanface.json\")\n",
    "#import_dataset(\"dataset/import_pratheepanfamily.json\")\n",
    "\n",
    "# abd has a native train/test splits\n",
    "#import_dataset(\"dataset/import_abd_te.json\")\n",
    "#import_dataset(\"dataset/import_abd_tr.json\")\n",
    "\n",
    "# vdm dataset is composed of 5 sub datasets with train/test splits\n",
    "#import_dataset(\"dataset/import_ami_te.json\")\n",
    "#import_dataset(\"dataset/import_ami_tr.json\")\n",
    "#import_dataset(\"dataset/import_ed_te.json\")\n",
    "#import_dataset(\"dataset/import_ed_tr.json\")\n",
    "#import_dataset(\"dataset/import_liris_te.json\")\n",
    "#import_dataset(\"dataset/import_liris_tr.json\")\n",
    "#import_dataset(\"dataset/import_ssg_te.json\")\n",
    "#import_dataset(\"dataset/import_ssg_tr.json\")\n",
    "#import_dataset(\"dataset/import_ut_te.json\")\n",
    "#import_dataset(\"dataset/import_ut_tr.json\")\n",
    "\n",
    "# schmugge dataset has really different filename formats but has a custom config file included\n",
    "schm = read_schmugge('dataset/Schmugge/data/.config.SkinImManager', 'dataset/Schmugge/data/data')\n",
    "process_schmugge(schm, 'dataset/Schmugge/data.csv', ori_out_dir='dataset/Schmugge/newdata/ori', gt_out_dir='dataset/Schmugge/newdata/gt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_hDPvMqQR2B"
   },
   "source": [
    "Import the train,validation,test sets txts used in the Skinny paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBnIeju53obQ",
    "outputId": "bacddcec-1186-4c43-a95b-33bbc15518b5"
   },
   "outputs": [],
   "source": [
    "# import ECU splits from the Skinny paper\n",
    "\n",
    "# unzip archive\n",
    "!unzip -j drive/MyDrive/datasets/sets/ECU_Skinny.zip -d dataset/ECU/\n",
    "\n",
    "# import splits\n",
    "import_split('dataset/ECU/data.csv', 'dataset/ECU/train.txt',\n",
    "             'dataset/ECU/data.csv', 'tr')\n",
    "import_split('dataset/ECU/data.csv', 'dataset/ECU/test.txt',\n",
    "            'dataset/ECU/data.csv', 'te')\n",
    "import_split('dataset/ECU/data.csv', 'dataset/ECU/val.txt',\n",
    "            'dataset/ECU/data.csv', 'va')\n",
    "\n",
    "\n",
    "\n",
    "# Import my HGR and Schmugge splits\n",
    "sch_from = 'drive/MyDrive/training/skinny/checkpoint-20210505-225202/schmugge_datacsv_model.csv' # sch\n",
    "hgr_from = 'drive/MyDrive/training/skinny/checkpoint-20210512-220723/HGR_data.csv' # hgr\n",
    "sch_to = 'dataset/Schmugge/data.csv'\n",
    "hgr_to = 'dataset/HGR_small/data.csv'\n",
    "\n",
    "\n",
    "!rm $sch_to\n",
    "!rm $hgr_to\n",
    "\n",
    "!cp $sch_from $sch_to\n",
    "!cp $hgr_from $hgr_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3jX-C5Guqa0"
   },
   "source": [
    "PYTHON Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQOz5t8rurx3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from xml.etree import ElementTree as ET\n",
    "from typing import Callable, Any\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aTkPRWSusO6"
   },
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n7IBE2busaS"
   },
   "outputs": [],
   "source": [
    "def inception_module(prev_layer, filters: int, activation=layers.LeakyReLU):\n",
    "    filters = filters // 4\n",
    "    conv_1 = layers.Conv2D(filters, (1, 1), padding='same')(prev_layer)\n",
    "    conv_1 = activation()(conv_1)\n",
    "    conv_3 = layers.Conv2D(filters, (1, 1), padding='same')(prev_layer)\n",
    "    conv_3 = layers.Conv2D(filters, (3, 3), padding='same')(conv_3)\n",
    "    conv_3 = activation()(conv_3)\n",
    "    conv_5 = layers.Conv2D(filters, (1, 1), padding='same')(prev_layer)\n",
    "    conv_5 = layers.Conv2D(filters, (5, 5), padding='same')(conv_5)\n",
    "    conv_5 = activation()(conv_5)\n",
    "    max_pool = layers.MaxPool2D(padding='same', strides=(1, 1))(prev_layer)\n",
    "    max_pool = layers.Conv2D(filters, (1, 1), padding='same')(max_pool)\n",
    "    max_pool = activation()(max_pool)\n",
    "    return tf.concat([conv_1, conv_3, conv_5, max_pool], axis=-1)\n",
    "\n",
    "\n",
    "def dense_block(prev_layer, filters: int, kernel_size: int or tuple, activation=layers.LeakyReLU):\n",
    "    dense_1 = layers.Conv2D(filters // 2, kernel_size, padding='same')(prev_layer)\n",
    "    dense_1 = layers.BatchNormalization()(dense_1)\n",
    "    dense_1 = activation()(dense_1)\n",
    "    dense_2 = layers.Conv2D(filters // 4, kernel_size, padding='same')(dense_1)\n",
    "    dense_2 = layers.BatchNormalization()(dense_2)\n",
    "    dense_2 = activation()(dense_2)\n",
    "    dense_3 = layers.Conv2D(filters // 8, kernel_size, padding='same')(dense_2)\n",
    "    dense_3 = layers.BatchNormalization()(dense_3)\n",
    "    dense_3 = activation()(dense_3)\n",
    "    return tf.concat([dense_1, dense_2, dense_3, prev_layer], axis=-1)\n",
    "\n",
    "\n",
    "def get_filters_count(level: int, initial_filters: int) -> int:\n",
    "    return 2**(level-1)*initial_filters\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, levels: int = None, initial_filters: int = None,\n",
    "                 image_channels: int = None, log_dir: str = 'logs',\n",
    "                 load_checkpoint: bool = False, checkpoint_extension: str = 'ckpt',\n",
    "                 model_name: str = None) -> None:\n",
    "        self.levels = levels\n",
    "        self.initial_filters = initial_filters\n",
    "        self.image_channels = image_channels\n",
    "        self.keras_model = None\n",
    "        self.log_dir = log_dir\n",
    "        self.checkpoint_extension = checkpoint_extension\n",
    "        if model_name is not None:\n",
    "            self.name = model_name\n",
    "        self.load_checkpoint = load_checkpoint\n",
    "        if not load_checkpoint and os.path.isdir(self.get_logdir()):\n",
    "            self.name = f\"{self.name}_{get_timestamp()}\"\n",
    "\n",
    "    def get_model(self) -> keras.Model:\n",
    "        path = os.path.join(self.log_dir, self.name, 'checkpoint', f'saved_model.{self.checkpoint_extension}')\n",
    "        \n",
    "        if self.load_checkpoint and self.checkpoint_extension != 'ckpt': # load full model\n",
    "            self.keras_model = load_model(path, compile=False)\n",
    "            return self.keras_model\n",
    "\n",
    "        self.keras_model = self.create_model()\n",
    "        self.__change_model_name()\n",
    "        self.__plot_model(self.keras_model)\n",
    "\n",
    "        if self.load_checkpoint: # load weights\n",
    "            try:\n",
    "                self.keras_model.load_weights(path)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        return self.keras_model\n",
    "\n",
    "    def __plot_model(self, model: keras.Model) -> None:\n",
    "        plot_model(model, to_file=os.path.join(self.log_dir, self.name, 'model.png'), show_shapes=True)\n",
    "\n",
    "    def __change_model_name(self) -> None:\n",
    "        if self.name is not None and self.keras_model is not None:\n",
    "            self.keras_model._name = self.name\n",
    "\n",
    "    def get_logdir(self) -> str:\n",
    "        return os.path.join(self.log_dir, self.name)\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_model(self) -> keras.Model:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Skinny(Model):\n",
    "    name = \"Skinny\"\n",
    "\n",
    "    def create_model(self) -> keras.Model:\n",
    "        self.levels += 1\n",
    "        kernel_size = (3, 3)\n",
    "        layers_list = [None for _ in range(self.levels)]\n",
    "        layers_list[0] = layers.Input(shape=(None, None, self.image_channels), name='feature')\n",
    "        activation = layers.LeakyReLU\n",
    "\n",
    "        for i in range(1, self.levels):\n",
    "            filters = get_filters_count(i, self.initial_filters)\n",
    "            prev = i-1\n",
    "            if i != 1:\n",
    "                layers_list[i] = layers.MaxPool2D()(layers_list[prev])\n",
    "                prev = i\n",
    "\n",
    "            layers_list[i] = layers.Conv2D(filters, kernel_size, padding='same')(layers_list[prev])\n",
    "            layers_list[i] = layers.BatchNormalization()(layers_list[i])\n",
    "            layers_list[i] = activation()(layers_list[i])\n",
    "            layers_list[i] = inception_module(layers_list[i], filters, activation)\n",
    "\n",
    "        for i in range(self.levels-2, 0, -1):\n",
    "            filters = get_filters_count(i, self.initial_filters)\n",
    "            layers_list[i+1] = layers.UpSampling2D()(layers_list[i+1])\n",
    "            layers_list[i+1] = layers.Conv2D(filters, kernel_size, padding='same')(layers_list[i+1])\n",
    "            layers_list[i+1] = layers.BatchNormalization()(layers_list[i+1])\n",
    "            layers_list[i+1] = activation()(layers_list[i+1])\n",
    "\n",
    "            layers_list[i] = dense_block(layers_list[i], filters, kernel_size, activation)\n",
    "\n",
    "            layers_list[i] = tf.concat([layers_list[i+1], layers_list[i]], axis=-1)\n",
    "            layers_list[i] = inception_module(layers_list[i], filters, activation)\n",
    "\n",
    "        layers_list[1] = layers.Conv2D(self.initial_filters, kernel_size, padding='same')(layers_list[1])\n",
    "        layers_list[1] = activation()(layers_list[1])\n",
    "        layers_list[1] = layers.Conv2D(self.initial_filters//2, kernel_size, padding='same')(layers_list[1])\n",
    "        layers_list[1] = activation()(layers_list[1])\n",
    "        layers_list[1] = layers.Conv2D(1, kernel_size, padding='same',\n",
    "                                       activation='sigmoid', name='label')(layers_list[1])\n",
    "\n",
    "        model = keras.Model(inputs=[layers_list[0]], outputs=[layers_list[1]], name=self.name)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr_qjZfqvErA"
   },
   "source": [
    "Define PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qZV2BhqvE6m"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.operations = [] # Preprocessor has no operations by default\n",
    "    \n",
    "    # adds a downscale operation to the preprocessing operations list, returns self\n",
    "    def downscale(self, max_pixel_count):\n",
    "        # operation that downscales the images composed of more pixels\n",
    "        # than max_pixel_count preserving the aspect ratio\n",
    "        def downscale_operation(data):\n",
    "            for k, v in data.items():\n",
    "                tensor_shape = tf.cast(tf.shape(v), tf.float32)\n",
    "                coefficient = max_pixel_count / (tensor_shape[0] * tensor_shape[1])\n",
    "                coefficient = tf.math.sqrt(coefficient)\n",
    "                data[k] = tf.cond(coefficient >= 1.0, lambda: v,\n",
    "                                  lambda: tf.image.resize(v, [tf.cast(tensor_shape[0] * coefficient, tf.uint16),\n",
    "                                                              tf.cast(tensor_shape[1] * coefficient, tf.uint16)]))\n",
    "            return data\n",
    "\n",
    "        self.operations.append(downscale_operation)\n",
    "        return self\n",
    "    \n",
    "    # adds a cast operation to the preprocessing operations list, returns self\n",
    "    def cast(self, dtype):\n",
    "        # operation that casts the images data into the given dtype\n",
    "        def cast_operation(data):\n",
    "            for k, v in data.items():\n",
    "                data[k] = tf.cast(v, dtype)\n",
    "            return data\n",
    "\n",
    "        self.operations.append(cast_operation)\n",
    "        return self\n",
    "    \n",
    "    # adds a normalize operation to the preprocessing operations list, returns self\n",
    "    def normalize(self):\n",
    "        # operation that transforms the images data from uint8(0-255 limited) into floats(0-1 limited)\n",
    "        def normalize_operation(data):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v / 255.0\n",
    "            return data\n",
    "\n",
    "        self.operations.append(normalize_operation)\n",
    "        return self\n",
    "    \n",
    "    # adds a padding operation to the preprocessing operations list, returns self\n",
    "    def pad(self, network_levels):\n",
    "        number_multiple = 2**(network_levels-1)\n",
    "        # operation that adds padding to the down and the right of the images\n",
    "        def padding_operation(data):\n",
    "            for k, v in data.items():\n",
    "                tensor_shape = tf.shape(v)\n",
    "                data[k] = tf.pad(v, [[0, number_multiple - tensor_shape[0] % number_multiple],\n",
    "                                     [0,  number_multiple - tensor_shape[1] % number_multiple],\n",
    "                                     [0, 0]])\n",
    "            return data\n",
    "\n",
    "        self.operations.append(padding_operation)\n",
    "        return self\n",
    "    \n",
    "    # executes all the operation functions defined in the Preprocessor instance\n",
    "    # on the given Dataset object and returns the transformed Dataset object\n",
    "    def add_to_graph(self, dataset) -> tf.data.Dataset:\n",
    "        for operation in self.operations:\n",
    "            dataset = dataset.map(operation) # map will execute one function on every element of the Dataset\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBucaN-ovFF-"
   },
   "source": [
    "Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jbs21davFLt"
   },
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    buffer_size = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    def __init__(self, dataset_dir: str, batch_size: int, preprocessor: Preprocessor = None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.val_dataset = None\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.preprocessor = Preprocessor() if preprocessor is None else preprocessor\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.__batch_and_prefetch(self.__train_dataset)\n",
    "\n",
    "    @train_dataset.setter\n",
    "    def train_dataset(self, dataset: tf.data.Dataset):\n",
    "        self.__train_dataset = dataset\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.__batch_and_prefetch(self.__test_dataset)\n",
    "\n",
    "    @test_dataset.setter\n",
    "    def test_dataset(self, dataset: tf.data.Dataset):\n",
    "        self.__test_dataset = dataset\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self):\n",
    "        return self.__batch_and_prefetch(self.__val_dataset)\n",
    "\n",
    "    @val_dataset.setter\n",
    "    def val_dataset(self, dataset: tf.data.Dataset):\n",
    "        self.__val_dataset = dataset\n",
    "\n",
    "    @property\n",
    "    def preprocessor(self) -> Preprocessor:\n",
    "        return self.__preprocessor\n",
    "\n",
    "    @preprocessor.setter\n",
    "    def preprocessor(self, preprocessor: Preprocessor):\n",
    "        self.__preprocessor = preprocessor\n",
    "        self.__reinstantiate()\n",
    "\n",
    "    def __reinstantiate(self):\n",
    "        self.train_dataset = self.__create_dataset_pipeline('tr')\n",
    "        self.val_dataset = self.__create_dataset_pipeline('va', shuffle=False)\n",
    "        self.test_dataset = self.__create_dataset_pipeline('te', shuffle=False)\n",
    "\n",
    "    def __batch_and_prefetch(self, dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "        return dataset.\\\n",
    "            padded_batch(self.batch_size, padded_shapes=({'feature': [None, None, 3]}, {'label': [None, None, 1]})).\\\n",
    "            prefetch(buffer_size=self.buffer_size)\n",
    "    \n",
    "    def __get_subset_paths(self, subset: str) -> list:\n",
    "        # read the images CSV (ori_image_filename.ext, gt_image_filename.ext)\n",
    "        file = open(os.path.join(self.dataset_dir, 'data.csv'))\n",
    "        file3c = file.read().splitlines()\n",
    "        file.close()\n",
    "\n",
    "        files = []\n",
    "\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            note = entry.split(csv_sep)[2]\n",
    "            \n",
    "            if note == subset:\n",
    "                files.append((ori_path, gt_path))\n",
    "        \n",
    "        if len(files) == 0:\n",
    "            print(f'''No files found for subset {subset}!\\n\n",
    "            using the whole dataset instead''')\n",
    "            for entry in file3c:\n",
    "                ori_path = entry.split(csv_sep)[0]\n",
    "                gt_path = entry.split(csv_sep)[1]\n",
    "                \n",
    "                files.append((ori_path, gt_path))\n",
    "        else:\n",
    "            print(f'Found {subset} split of {len(files)} files')\n",
    "        \n",
    "        return files\n",
    "\n",
    "    def __create_dataset_pipeline(self, subset: str, shuffle: bool = True) -> tf.data.Dataset:\n",
    "        def process_example_paths(example):\n",
    "            # TODO bmp should raise errors, tensorflow doesn't like them\n",
    "            return {'feature': tf.io.decode_image(tf.io.read_file(example[0]), channels=3, expand_animations = False),\n",
    "                    'label': tf.io.decode_image(tf.io.read_file(example[1]), channels=1,  expand_animations = False)}\n",
    "\n",
    "        def convert_to_in_out_dicts(example):\n",
    "            output_dict = {'label': example.pop('label')}\n",
    "            return example, output_dict\n",
    "\n",
    "        dataset = self.__get_subset_paths(subset)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "        dataset = dataset.map(process_example_paths)\n",
    "        dataset = self.preprocessor.add_to_graph(dataset)\n",
    "        dataset = dataset.map(convert_to_in_out_dicts).cache()\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(2000, reshuffle_each_iteration=True)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlOJNVGduUSZ"
   },
   "outputs": [],
   "source": [
    "# custom loader used for predictions\n",
    "\n",
    "def my_batch(dataset: tf.data.Dataset, batch_size) -> tf.data.Dataset:\n",
    "    return dataset.\\\n",
    "        padded_batch(batch_size, padded_shapes=({'feature': [None, None, 3]})).\\\n",
    "        prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def my_file(path: str) -> list:\n",
    "    files = [path]\n",
    "    return files\n",
    "\n",
    "def my_loader(path, preprocessor) -> tf.data.Dataset:\n",
    "    def my_process(example):\n",
    "        return {'feature': tf.io.decode_image(tf.io.read_file(example), channels=3, expand_animations = False)}\n",
    "    # def my_inout(example):\n",
    "    #     output_dict = {'label': example.pop('label')}\n",
    "    #     return example, output_dict\n",
    "\n",
    "    #dataset = [(path),]\n",
    "    dataset = my_file(path)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.map(my_process)\n",
    "\n",
    "    t_start = time.time()\n",
    "    # preprocessing\n",
    "    dataset = preprocessor.add_to_graph(dataset)\n",
    "    t_elapsed = time.time() - t_start\n",
    "\n",
    "    #dataset = dataset.map(my_inout).cache()\n",
    "    dataset = dataset.cache()\n",
    "    return dataset, t_elapsed\n",
    "\n",
    "def single_predict(model, im_path: str, out_path: str, preprocessor):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok = True)\n",
    "\n",
    "    # get tf Dataset structure containing the image to predict and elapsed preprocessing time\n",
    "    tf_ds, t_elapsed_pre = my_loader(im_path, preprocessor)\n",
    "\n",
    "    # predict the image\n",
    "    for entry in tf_ds:\n",
    "        # convert to tensor to prevent memory leak https://stackoverflow.com/a/64765018\n",
    "        tensor = tf.convert_to_tensor(entry['feature'], dtype=tf.float32)\n",
    "        tensor = tf.expand_dims(tensor, axis=0) # add a dimension\n",
    "\n",
    "        #model = my_load_model(model_path)\n",
    "        model.get_model()\n",
    "\n",
    "        # get time before prediction\n",
    "        t_start = time.time()\n",
    "\n",
    "        pred = model.keras_model.predict(tensor) # predict from feature image (X)\n",
    "        # post-processing\n",
    "        pred = pred[0]*255 # reshape and de-preprocess\n",
    "\n",
    "        # prediction + postprocessing\n",
    "        t_elapsed = time.time() - t_start\n",
    "\n",
    "        # preprocessing + prediction + postprocessing elapsed time\n",
    "        t_elapsed_full = t_elapsed_pre + t_elapsed\n",
    "\n",
    "        # save to a file\n",
    "        cv2.imwrite(out_path, pred)\n",
    "\n",
    "        print(t_elapsed_pre)\n",
    "        print(t_elapsed)\n",
    "        print(t_elapsed_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf0u4as4vpTs"
   },
   "source": [
    "Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2ehVveyvpbB"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, data_loader: DataLoader, model: Model,\n",
    "                 log_dir: str = './logs', evaluate_test_data=False):\n",
    "        self.data_loader = data_loader\n",
    "        self.model = model\n",
    "        self.metrics = []\n",
    "        self.losses = []\n",
    "        self.callbacks = []\n",
    "        self.log_dir = os.path.join(log_dir, model.name)\n",
    "        self.timelog = None\n",
    "        self.evaluate_test_data = evaluate_test_data\n",
    "\n",
    "    @property\n",
    "    def model(self) -> Model:\n",
    "        return self.__model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, model: Model):\n",
    "        self.__model = model\n",
    "\n",
    "    @property\n",
    "    def data_loader(self) -> DataLoader:\n",
    "        return self.__data_loader\n",
    "\n",
    "    @data_loader.setter\n",
    "    def data_loader(self, data_loader: DataLoader):\n",
    "        self.__data_loader = data_loader\n",
    "\n",
    "    def add_metrics(self, metrics):\n",
    "        if type(metrics) is not list:\n",
    "            metrics = [metrics]\n",
    "        for metric in metrics:\n",
    "            self.metrics.append(metric)\n",
    "\n",
    "    def add_losses(self, losses) -> None:\n",
    "        if type(losses) is not list:\n",
    "            losses = [losses]\n",
    "        for loss in losses:\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def add_callbacks(self, callbacks) -> None:\n",
    "        if type(callbacks) is not list:\n",
    "            callbacks = [callbacks]\n",
    "        for callback in callbacks:\n",
    "            self.callbacks.append(callback)\n",
    "\n",
    "    def combined_loss(self):\n",
    "        def loss(y_true, y_pred):\n",
    "            result = None\n",
    "            for i, v in enumerate(self.losses):\n",
    "                if i == 0:\n",
    "                    result = v(y_true, y_pred)\n",
    "                else:\n",
    "                    result += v(y_true, y_pred)\n",
    "            return result\n",
    "        return loss\n",
    "\n",
    "    def __log_evaluation_metrics(self, metrics: dict):\n",
    "        root = ET.Element('metrics')\n",
    "        tree = ET.ElementTree(root)\n",
    "        for name, value in metrics.items():\n",
    "            metric_element = ET.SubElement(root, name)\n",
    "            metric_element.text = str(value)\n",
    "        tree.write(open(os.path.join(self.model.get_logdir(), 'test_metrics.xml'), 'w'), encoding='unicode')\n",
    "\n",
    "    def train(self, epochs, optimizer, initial_epoch=0, verbose=1):\n",
    "        assert self.model is not None, \"Model hasn't been set for the trainer.\"\n",
    "        assert self.data_loader is not None, \"DataLoader hasn't been set for the trainer.\"\n",
    "        os.makedirs(self.model.get_logdir(), exist_ok=True)\n",
    "        model = self.model.get_model()\n",
    "        model.compile(optimizer=optimizer, loss=self.combined_loss(), metrics=self.metrics)\n",
    "\n",
    "        model.fit(self.data_loader.train_dataset, validation_data=self.data_loader.val_dataset,\n",
    "                  epochs=epochs, verbose=verbose, initial_epoch=initial_epoch,\n",
    "                  callbacks=self.callbacks, shuffle=True)\n",
    "        if self.evaluate_test_data:\n",
    "            evaluation_metrics = model.evaluate(self.data_loader.test_dataset, verbose=1)\n",
    "            evaluation_metrics = dict(zip(model.metrics_names, evaluation_metrics))\n",
    "            self.__log_evaluation_metrics(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pap7Ajyvpg3"
   },
   "source": [
    "Define WorkScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IdmW9BBvplP"
   },
   "outputs": [],
   "source": [
    "class WorkScheduler:\n",
    "    def __init__(self) -> None:\n",
    "        self.data = []\n",
    "\n",
    "    def add_data(self, model: Model,\n",
    "                 func: Callable[[Model, Any], None],\n",
    "                 **kwargs) -> None:\n",
    "        self.data.append((model, func, kwargs))\n",
    "\n",
    "    def do_work(self) -> None:\n",
    "        for model, func, kwargs in self.data:\n",
    "            tf.keras.backend.clear_session()\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "            func(model=model, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfzWKQCiv4bW"
   },
   "source": [
    "Define losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orSZWReTv4ki"
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, epsilon=1e-15):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=(1, 2, 3))\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=(1, 2, 3))\n",
    "    loss = tf.squeeze(tf.reshape(1 - numerator / denominator, (-1, 1, 1)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ovs--81wa5V"
   },
   "source": [
    "Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GI9TWN8Uwa_R"
   },
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_score = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_score = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision_score = precision(y_true, y_pred)\n",
    "    recall_score = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_score * recall_score) / (precision_score + recall_score + K.epsilon()))\n",
    "\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    intersection = y_true * y_pred\n",
    "    not_true = 1 - y_true\n",
    "    union = y_true + (not_true * y_pred)\n",
    "\n",
    "    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIUBDDWbwbJm"
   },
   "source": [
    "Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eVAaUSPwbNf"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ModelCheckpoint(keras.callbacks.ModelCheckpoint, CustomCallback):\n",
    "    checkpoint_name = 'saved_model.ckpt'\n",
    "\n",
    "    def set_model(self, model):\n",
    "        dir_name = os.path.join(self.filepath, 'checkpoint')\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        timestr = get_timestamp()\n",
    "        self.filepath = os.path.join(F\"/content/drive/MyDrive/training/skinny/checkpoint-\" + timestr, self.checkpoint_name )\n",
    "        #self.filepath = os.path.join(F\"/content/drive/MyDrive/training/skinny/dark/checkpoint-\" + timestr, self.checkpoint_name )\n",
    "        super().set_model(model)\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(keras.callbacks.ReduceLROnPlateau, CustomCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ProgbarLogger(keras.callbacks.ProgbarLogger, CustomCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "class EarlyStopping(keras.callbacks.EarlyStopping, CustomCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TensorBoard(keras.callbacks.TensorBoard, CustomCallback):\n",
    "    def set_model(self, model):\n",
    "        self.log_dir = os.path.join(self.log_dir, 'tensorboard')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        super().set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63IdHK1swkns"
   },
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nK4NHke1wkt3",
    "outputId": "8fba4b8a-1c97-4d95-b606-4bcf7e0be3b5"
   },
   "outputs": [],
   "source": [
    "# Skinny (full model with inception and dense blocks)\n",
    "model_name = 'Skinny'\n",
    "\n",
    "# model settings\n",
    "levels = 6\n",
    "initial_filters = 19\n",
    "image_channels = 3\n",
    "\n",
    "# train settings\n",
    "max_epochs = 200\n",
    "initial_lr = 1e-4\n",
    "batch_size = 3\n",
    "\n",
    "# dirs\n",
    "log_dir = 'logs'\n",
    "# in the dataset_dir there is a csv file containing all the splits data\n",
    "#dataset_dir = 'dataset/Schmugge' #'dataset/ECU'\n",
    "\n",
    "# preprocessing operations\n",
    "preprocessor = Preprocessor()\n",
    "preprocessor.cast(dtype=tf.float32).normalize().downscale(max_pixel_count=512**2).pad(network_levels=levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYfFoxUQwkyc"
   },
   "source": [
    "Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihDiykfwk2v"
   },
   "outputs": [],
   "source": [
    "def train_function(model: Model, batch_size: int, dataset_dir: str) -> None:\n",
    "    data_loader = DataLoader(dataset_dir=dataset_dir, batch_size=batch_size, preprocessor=preprocessor)\n",
    "    trainer = Trainer(data_loader=data_loader, model=model, evaluate_test_data=True)\n",
    "    trainer.add_losses([K.binary_crossentropy, dice_loss])\n",
    "    trainer.add_metrics([\n",
    "        f1,\n",
    "        iou,\n",
    "        precision,\n",
    "        recall\n",
    "    ])\n",
    "    trainer.add_callbacks([\n",
    "        ModelCheckpoint(filepath=model.get_logdir(), verbose=1, save_best_only=True,\n",
    "                                  monitor='val_f1', mode='max', save_weight_only=False),#save_weights_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_f1', factor=0.5, verbose=1, mode='max', min_lr=1e-6, patience=5),\n",
    "        #EarlyStopping(monitor='val_f1', mode='max', patience=10, verbose=1),\n",
    "        EarlyStopping(monitor='val_f1', mode='max', patience=50, verbose=1), # for dark\n",
    "        TensorBoard(log_dir=model.get_logdir(), histogram_freq=5)\n",
    "    ])\n",
    "\n",
    "    trainer.train(max_epochs, tf.keras.optimizers.Adam(learning_rate=initial_lr), verbose=1)\n",
    "\n",
    "def test_function(model: Model, dataset_dir: str) -> None:\n",
    "    data_loader = DataLoader(dataset_dir=dataset_dir, batch_size=1, preprocessor=preprocessor)\n",
    "    trainer = Trainer(data_loader=data_loader, model=model, evaluate_test_data=True)\n",
    "    trainer.add_losses([K.binary_crossentropy, dice_loss])\n",
    "    trainer.add_metrics([\n",
    "        f1,\n",
    "        iou,\n",
    "        precision,\n",
    "        recall\n",
    "    ])\n",
    "\n",
    "    model.get_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    model.keras_model.compile(optimizer=optimizer, loss=trainer.combined_loss(), metrics=trainer.metrics)\n",
    "    evaluation_metrics = model.keras_model.evaluate(data_loader.test_dataset, verbose=1)\n",
    "    evaluation_metrics = dict(zip(model.keras_model.metrics_names, evaluation_metrics))\n",
    "    print(evaluation_metrics)\n",
    "\n",
    "# save Skinny X preprocessed-images (512**2) to files\n",
    "# save the images after the preprocessing, before they enter the model\n",
    "def save_x(model: Model, dataset_dir: str, preprocessor: Preprocessor, out_dir: str = 'x', skip = 0) -> None:\n",
    "    data_loader = DataLoader(dataset_dir=dataset_dir, batch_size=1, preprocessor=preprocessor)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "    i = skip\n",
    "    for entry in data_loader.test_dataset:\n",
    "        i += 1\n",
    "\n",
    "        # dict: {'feature': data, dtype=float32}\n",
    "        entry = entry[0]\n",
    "        # shape=(1, 320, 384, 1) dtype=float32\n",
    "        entry = entry['feature']\n",
    "        # reshape(320, 384, 3) and de-preprocess\n",
    "        entry = entry[0]*255\n",
    "        # convert to numpy array or cv2.imwrite doesn't work\n",
    "        entry = np.array(entry)\n",
    "\n",
    "        # cv2 works with BGR, not RGB\n",
    "        entry = cv2.cvtColor(entry, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # save to a file\n",
    "        filename = f\"{i}.png\"\n",
    "        cv2.imwrite(os.path.join(out_dir, filename), entry)\n",
    "        #tf.keras.preprocessing.image.save_img(os.path.join(out_dir, filename), entry)\n",
    "        # https://stackoverflow.com/a/61041738\n",
    "\n",
    "# save Skinny Y preprocessed-images (512**2) to files\n",
    "# save the images after the preprocessing, before they enter the model\n",
    "def save_y(model: Model, dataset_dir: str, preprocessor: Preprocessor, out_dir: str = 'y', skip = 0) -> None:\n",
    "    data_loader = DataLoader(dataset_dir=dataset_dir, batch_size=1, preprocessor=preprocessor)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "    i = skip\n",
    "    for entry in data_loader.test_dataset:\n",
    "        i += 1\n",
    "\n",
    "        # dict: {'label': data, dtype=float32}\n",
    "        entry = entry[1]\n",
    "        # shape=(1, 320, 384, 1) dtype=float32\n",
    "        entry = entry['label']\n",
    "        # reshape(320, 384, 1) and de-preprocess\n",
    "        entry = entry[0]*255\n",
    "        # convert to numpy array or cv2.imwrite doesn't work\n",
    "        entry = np.array(entry)\n",
    "\n",
    "        # save to a file\n",
    "        filename = f\"{i}.png\"\n",
    "        cv2.imwrite(os.path.join(out_dir, filename), entry)\n",
    "\n",
    "def predict_function(model: Model, dataset_dir: str, out_dir: str, skip = 0) -> None:\n",
    "    data_loader = DataLoader(dataset_dir=dataset_dir, batch_size=1, preprocessor=preprocessor)\n",
    "\n",
    "    model.get_model()\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "    i = skip\n",
    "    for entry in data_loader.test_dataset: # prova a predirre le immagini di test\n",
    "        i += 1\n",
    "\n",
    "        # dict: {'feature': data, 'types': float32}\n",
    "        entry = entry[0]\n",
    "\n",
    "        # convert to tensor to prevent memory leak https://stackoverflow.com/a/64765018\n",
    "        tensor = tf.convert_to_tensor(entry['feature'], dtype=tf.float32)\n",
    "        #pred = model.keras_model.predict(tensor) # predict from feature image (X)\n",
    "        pred = model.keras_model(tensor) # predict from feature image (X)\n",
    "        #print(pred)\n",
    "\n",
    "        pred = pred[0]*255 # reshape and de-preprocess\n",
    "        #print(pred)\n",
    "        pred = pred.numpy()\n",
    "\n",
    "        # save to a file\n",
    "        filename = f\"{i}.png\"\n",
    "        cv2.imwrite(os.path.join(out_dir, filename), pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvDuS90rQgNY"
   },
   "source": [
    "Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KjanqfQn1YY"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_filepath):\n",
    "    out = None\n",
    "    ext = os.path.splitext(checkpoint_filepath)[1]\n",
    "\n",
    "    if ext == '.chkp':\n",
    "        load_chkpt(checkpoint_filepath)\n",
    "        out = 'chkp'\n",
    "    elif ext == '.pb':\n",
    "        load_pb(checkpoint_filepath)\n",
    "        out = 'pb'\n",
    "    else:\n",
    "        print(f'Unknown model filetype: {ext}')\n",
    "    \n",
    "    return out\n",
    "\n",
    "def load_chkp(chkp_index_filepath):\n",
    "    # clear checkpoints\n",
    "    !rm -rf logs/Skinny/checkpoint\n",
    "\n",
    "    # define from/to loading directories\n",
    "    from_path = os.path.join(os.path.dirname(chkp_index_filepath), '.')\n",
    "    to_path = 'logs/Skinny/checkpoint'\n",
    "\n",
    "    # make default model folder\n",
    "    !mkdir -p $to_path\n",
    "\n",
    "    # copy the model files\n",
    "    !cp -r $from_path $to_path\n",
    "\n",
    "def load_pb(pb_filepath):\n",
    "    # clear checkpoints\n",
    "    !rm -rf logs/Skinny/checkpoint\n",
    "\n",
    "    # define from/to loading directories\n",
    "    from_path = os.path.join(os.path.dirname(pb_filepath), '.')\n",
    "    to_path = 'logs/Skinny/checkpoint/saved_model.pb'\n",
    "\n",
    "    # make default model folder\n",
    "    !mkdir -p $to_path\n",
    "\n",
    "    # copy the model files\n",
    "    !cp -r $from_path $to_path\n",
    "\n",
    "def load_schmugge_skintone_split(skintone):\n",
    "    !rm dataset/Schmugge/data.csv\n",
    "\n",
    "    if skintone == 'dark':\n",
    "        !cp drive/MyDrive/training/skinny/checkpoint-20210523-110554/dark2305_1309.csv dataset/Schmugge/data.csv\n",
    "        print(f'{skintone}(sch) split imported!')\n",
    "    elif skintone == 'medium':\n",
    "        !cp drive/MyDrive/training/skinny/checkpoint-20210523-112308/medium2305_1323.csv dataset/Schmugge/data.csv\n",
    "        print(f'{skintone}(sch) split imported!')\n",
    "    elif skintone == 'light':\n",
    "        !cp drive/MyDrive/training/skinny/checkpoint-20210523-122027/light2305_1420.csv dataset/Schmugge/data.csv\n",
    "        print(f'{skintone}(sch) split imported!')\n",
    "    else:\n",
    "        print(f'no split found for (sch) skintone: {skintone}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSt7qY8pxpCW"
   },
   "outputs": [],
   "source": [
    "# Add some already existing trained models to a dictionary for ease of access\n",
    "\n",
    "models = {}\n",
    "\n",
    "models['ecu'] = 'drive/MyDrive/training/skinny/checkpoint-20210428-155148/saved_model.ckpt/saved_model.pb' # ecu\n",
    "models['schmugge'] = 'drive/MyDrive/training/skinny/checkpoint-20210505-225202/saved_model.ckpt/saved_model.pb' # sch\n",
    "models['hgr'] = 'drive/MyDrive/training/skinny/checkpoint-20210512-220723/saved_model.ckpt/saved_model.pb' # hgr\n",
    "\n",
    "models['dark'] = 'drive/MyDrive/training/skinny/checkpoint-20210523-110554/saved_model.ckpt/saved_model.pb' # dark\n",
    "models['medium'] = 'drive/MyDrive/training/skinny/checkpoint-20210523-112308/saved_model.ckpt/saved_model.pb' # medium\n",
    "models['light'] = 'drive/MyDrive/training/skinny/checkpoint-20210523-122027/saved_model.ckpt/saved_model.pb' # light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeeLfroJyVMS"
   },
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0s_UkxKlnvN",
    "outputId": "cf3779d2-9154-4ee4-873d-781eeed20bd2"
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset/Schmugge'\n",
    "\n",
    "#mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True, model_name=model_name)\n",
    "mod = Skinny(levels, initial_filters, image_channels, log_dir)  # creates new model\n",
    "\n",
    "#self.keras_model = load_model(path, compile=False)\n",
    "#mod = tf.saved_model.load('models')\n",
    "\n",
    "scheduler = WorkScheduler()\n",
    "scheduler.add_data(mod, train_function, batch_size = batch_size, dataset_dir = dataset_dir)\n",
    "scheduler.do_work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8cAzXpwyq_T"
   },
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "V_3XkHNzyrOq",
    "outputId": "03bf66b8-2d61-4e4a-854c-7b697b51c724"
   },
   "outputs": [],
   "source": [
    "#chkp_ext = 'pb' # pb or chkp\n",
    "dataset_dir = 'dataset/abd-skin'\n",
    "\n",
    "mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "             model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "scheduler = WorkScheduler()\n",
    "scheduler.add_data(mod, test_function, dataset_dir = dataset_dir)\n",
    "scheduler.do_work()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmb_fL8mTQ7o"
   },
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbRzgInCPH2K"
   },
   "outputs": [],
   "source": [
    "def cross_predict(train_db, predict_db, timestr = None, save = False):\n",
    "    is_ecu = False\n",
    "    if predict_db == 'ecu':\n",
    "        predict_db = 'dataset/ECU'\n",
    "        is_ecu = True\n",
    "    elif predict_db == 'hgr':\n",
    "        predict_db = 'dataset/HGR_small'\n",
    "    elif predict_db == 'schmugge':\n",
    "        predict_db = 'dataset/Schmugge'\n",
    "    \n",
    "    if timestr == None:\n",
    "        timestr = get_timestamp()\n",
    "\n",
    "    # set the whole dataset as the testing set\n",
    "    whole_test = os.path.join(predict_db, 'data.csv') # dataset to process\n",
    "\n",
    "    if is_ecu:\n",
    "        csv_count_test(whole_test, 2000, 'te') # limit ecu to 2000 predictions or ram crash, do it 2 times\n",
    "    else:\n",
    "        csv_full_test(whole_test, 'te')\n",
    "\n",
    "    # load model files\n",
    "    chkp_ext = load_checkpoint(models[train_db])\n",
    "    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "            model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "    # predict\n",
    "    ds_name = os.path.basename(predict_db).lower()\n",
    "    if ds_name == 'hgr_small':\n",
    "        ds_name = 'hgr'\n",
    "\n",
    "\n",
    "    out_dir = os.path.join(timestr, 'skinny', 'cross', f'{train_db}_on_{ds_name}')\n",
    "    x_dir = os.path.join(out_dir, 'x')\n",
    "    y_dir = os.path.join(out_dir, 'y')\n",
    "    pred_dir = os.path.join(out_dir, 'p')\n",
    "\n",
    "    scheduler = WorkScheduler()\n",
    "    scheduler.add_data(None, save_x, dataset_dir = predict_db, out_dir = x_dir, preprocessor = preprocessor)\n",
    "    scheduler.add_data(None, save_y, dataset_dir = predict_db, out_dir = y_dir,  preprocessor = preprocessor)\n",
    "    scheduler.add_data(mod, predict_function, dataset_dir = predict_db, out_dir = pred_dir)\n",
    "    scheduler.do_work()\n",
    "\n",
    "    if is_ecu: # time to do second half\n",
    "        csv_not_test(whole_test)\n",
    "        scheduler = WorkScheduler()\n",
    "        scheduler.add_data(None, save_x, dataset_dir = predict_db, out_dir = x_dir, preprocessor = preprocessor, skip = 2000)\n",
    "        scheduler.add_data(None, save_y, dataset_dir = predict_db, out_dir = y_dir,  preprocessor = preprocessor, skip = 2000)\n",
    "        scheduler.add_data(mod, predict_function, dataset_dir = predict_db, out_dir = pred_dir, skip = 2000)\n",
    "        scheduler.do_work()\n",
    "\n",
    "\n",
    "    # zip and save predictions\n",
    "    if save:\n",
    "        zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "        !zip -r $zip_path $timestr\n",
    "\n",
    "\n",
    "def base_predict(db_name, timestr = None, save = False):\n",
    "    if db_name == 'ecu':\n",
    "        ds = 'dataset/ECU'\n",
    "    elif db_name == 'hgr':\n",
    "        ds = 'dataset/HGR_small'\n",
    "    elif db_name == 'schmugge':\n",
    "        ds = 'dataset/Schmugge'\n",
    "    \n",
    "    if timestr == None:\n",
    "        timestr = get_timestamp()\n",
    "    \n",
    "    # load model files\n",
    "    chkp_ext = load_checkpoint(models[db_name])\n",
    "    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "            model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "\n",
    "    out_dir = os.path.join(timestr, 'skinny', 'base', db_name)\n",
    "    x_dir = os.path.join(out_dir, 'x')\n",
    "    y_dir = os.path.join(out_dir, 'y')\n",
    "    pred_dir = os.path.join(out_dir, 'p')\n",
    "\n",
    "    scheduler = WorkScheduler()\n",
    "    scheduler.add_data(None, save_x, dataset_dir = ds, out_dir = x_dir, preprocessor = preprocessor)\n",
    "    scheduler.add_data(None, save_y, dataset_dir = ds, out_dir = y_dir,  preprocessor = preprocessor)\n",
    "    scheduler.add_data(mod, predict_function, dataset_dir = ds, out_dir = pred_dir)\n",
    "    scheduler.do_work()\n",
    "\n",
    "    # zip and save predictions\n",
    "    if save:\n",
    "        zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "        !zip -r $zip_path $timestr\n",
    "\n",
    "\n",
    "def cross_predict_skintones(train_skintone, predict_skintone, timestr = None, save = False):\n",
    "    db_dir = 'dataset/Schmugge'\n",
    "\n",
    "    if timestr == None:\n",
    "        timestr = get_timestamp()\n",
    "\n",
    "    # update the csv file to set the prediction set\n",
    "    gen_sch_by_skintone(predict_skintone, 'test')\n",
    "\n",
    "    # load model files\n",
    "    chkp_ext = load_checkpoint(models[train_skintone])\n",
    "    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "            model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "    # predict\n",
    "\n",
    "    out_dir = os.path.join(timestr, 'skinny', 'cross', f'{train_skintone}_on_{predict_skintone}')\n",
    "    x_dir = os.path.join(out_dir, 'x')\n",
    "    y_dir = os.path.join(out_dir, 'y')\n",
    "    pred_dir = os.path.join(out_dir, 'p')\n",
    "\n",
    "    scheduler = WorkScheduler()\n",
    "    scheduler.add_data(None, save_x, dataset_dir = db_dir, out_dir = x_dir, preprocessor = preprocessor)\n",
    "    scheduler.add_data(None, save_y, dataset_dir = db_dir, out_dir = y_dir,  preprocessor = preprocessor)\n",
    "    scheduler.add_data(mod, predict_function, dataset_dir = db_dir, out_dir = pred_dir)\n",
    "    scheduler.do_work()\n",
    "\n",
    "    # zip and save predictions\n",
    "    if save:\n",
    "        zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "        !zip -r $zip_path $timestr\n",
    "\n",
    "def base_predict_skintones(skintone, timestr = None, save = False):\n",
    "    db_dir = 'dataset/Schmugge'\n",
    "\n",
    "    if timestr == None:\n",
    "        timestr = get_timestamp()\n",
    "    \n",
    "    # load skintone split\n",
    "    load_schmugge_skintone_split(skintone)\n",
    "    \n",
    "    # load model files\n",
    "    chkp_ext = load_checkpoint(models[skintone])\n",
    "    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "            model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "\n",
    "    out_dir = os.path.join(timestr, 'skinny', 'base', skintone)\n",
    "    x_dir = os.path.join(out_dir, 'x')\n",
    "    y_dir = os.path.join(out_dir, 'y')\n",
    "    pred_dir = os.path.join(out_dir, 'p')\n",
    "\n",
    "    scheduler = WorkScheduler()\n",
    "    scheduler.add_data(None, save_x, dataset_dir = db_dir, out_dir = x_dir, preprocessor = preprocessor)\n",
    "    scheduler.add_data(None, save_y, dataset_dir = db_dir, out_dir = y_dir,  preprocessor = preprocessor)\n",
    "    scheduler.add_data(mod, predict_function, dataset_dir = db_dir, out_dir = pred_dir)\n",
    "    scheduler.do_work()\n",
    "\n",
    "    # zip and save predictions\n",
    "    if save:\n",
    "        zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "        !zip -r $zip_path $timestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8a0vl2eWcbG",
    "outputId": "d4969ccb-3461-4c1d-ab41-1422e1462514"
   },
   "outputs": [],
   "source": [
    "mode = 'skintones' # 'normal' or 'skintones'\n",
    "\n",
    "\n",
    "timestr = get_timestamp()\n",
    "\n",
    "if mode == 'normal':\n",
    "    modls = ['ecu', 'hgr', 'schmugge']\n",
    "\n",
    "    # base predictions: based on splits defined by me and only predict on self\n",
    "    # timestr/skinny/base/{ecu,hgr,schmugge}/{p/y/x}\n",
    "    for ds_name in modls:\n",
    "        base_predict(ds_name, timestr = timestr, save = False)\n",
    "\n",
    "    # cross predictions: use a dataset whole as the testing set\n",
    "    # timestr/skinny/cross/ecu_on_ecu/{p/y/x}\n",
    "    for ds_train in modls:\n",
    "        for ds_test in modls:\n",
    "            cross_predict(ds_train, ds_test, timestr = timestr, save = False)\n",
    "\n",
    "    # zip and save predictions\n",
    "    zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "    !zip -r $zip_path $timestr\n",
    "elif mode == 'skintones':\n",
    "    modls = ['light', 'medium', 'dark']\n",
    "\n",
    "    # base predictions: based on splits defined by me and only predict on self\n",
    "    # timestr/skinny/base/{ecu,hgr,schmugge}/{p/y/x}\n",
    "    for ds_name in modls:\n",
    "        base_predict_skintones(ds_name, timestr = timestr, save = False)\n",
    "\n",
    "    # cross predictions: use a dataset whole as the testing set\n",
    "    # timestr/skinny/cross/ecu_on_ecu/{p/y/x}\n",
    "    for ds_train in modls:\n",
    "        for ds_test in modls:\n",
    "            cross_predict_skintones(ds_train, ds_test, timestr = timestr, save = False)\n",
    "\n",
    "    # zip and save predictions\n",
    "    zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "    !zip -r $zip_path $timestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edpVj0bmxBXt",
    "outputId": "4f736196-bc8f-4657-a595-2f0dedf3ba00"
   },
   "outputs": [],
   "source": [
    "timestr = get_timestamp()\n",
    "skintones = ['light', 'medium', 'dark']\n",
    "ds = 'dataset/Schmugge'\n",
    "\n",
    "#for sk_tr in skintones:\n",
    "for sk_tr in ['dark']:\n",
    "    # load training model\n",
    "    chkp_ext = load_checkpoint(models[sk_tr])\n",
    "\n",
    "     # predict on train set\n",
    "    for sk_te in skintones:\n",
    "        out_dir = f'{sk_tr}_on_{sk_te}'\n",
    "        x_dir = os.path.join(timestr, out_dir, 'x')\n",
    "        y_dir = os.path.join(timestr, out_dir, 'y')\n",
    "        pred_dir = os.path.join(timestr, out_dir, 'p')\n",
    "\n",
    "        # Prepare test set\n",
    "        # re-import Schmugge\n",
    "        schm = read_schmugge('dataset/Schmugge/data/.config.SkinImManager', 'dataset/Schmugge/data/data')\n",
    "        process_schmugge(schm, 'dataset/Schmugge/data.csv', ori_out_dir='dataset/Schmugge/newdata/ori', gt_out_dir='dataset/Schmugge/newdata/gt')\n",
    "        filter_by_skintone_type = sk_te # light medium dark nd\n",
    "        filter_by_skintone_csv = 'dataset/Schmugge/data.csv' # dataset to process\n",
    "        filter_mode = 'test'\n",
    "        # filter to create testing set\n",
    "        csv_skintone_filter(filter_by_skintone_csv, filter_by_skintone_type, mode = filter_mode)\n",
    "        csv_skintone_count(filter_by_skintone_csv, filter_by_skintone_type)\n",
    "\n",
    "        # Predict\n",
    "        mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "                model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "\n",
    "        scheduler = WorkScheduler()\n",
    "        scheduler.add_data(None, save_x, dataset_dir = ds, out_dir = x_dir, preprocessor = preprocessor)\n",
    "        scheduler.add_data(None, save_y, dataset_dir = ds, out_dir = y_dir,  preprocessor = preprocessor)\n",
    "        scheduler.add_data(mod, predict_function, dataset_dir = ds, out_dir = pred_dir)\n",
    "        scheduler.do_work()\n",
    "\n",
    "# zip and save predictions\n",
    "zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_p.zip'\n",
    "!zip -r $zip_path $timestr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHyItKui-eRW",
    "outputId": "ac60485f-19e3-4302-de4b-046b7e30e52e"
   },
   "outputs": [],
   "source": [
    "# Attempts to single predict\n",
    "\n",
    "# preprocessing operations\n",
    "preprocessor = Preprocessor()\n",
    "preprocessor.cast(dtype=tf.float32).normalize().downscale(max_pixel_count=512**2).pad(network_levels=levels)\n",
    "\n",
    "chkp_ext = load_checkpoint(models['ecu'])\n",
    "#model_dir = 'logs/Skinny/checkpoint/saved_model.pb'\n",
    "inim = 'dataset/ECU/origin_images/im00001.jpg'\n",
    "outim = 'spred/im00001.png'\n",
    "\n",
    "mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True,\n",
    "        model_name=model_name, checkpoint_extension=chkp_ext)\n",
    "single_predict(mod, inim, outim, preprocessor)\n",
    "#prediction = get_prediction('dataset/org/features/im00043.jpg', preprocessor, my_load_model('logs/Skinny2/checkpoint/saved_model.ckpt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JYAPBfdIxs8"
   },
   "source": [
    "SAVE Xs and Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gugmyfXYI09P"
   },
   "outputs": [],
   "source": [
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "timestr = get_timestamp()\n",
    "x_dir = os.path.join(timestr, 'x')\n",
    "y_dir = os.path.join(timestr, 'y')\n",
    "\n",
    "# all but normalize, TODO right?\n",
    "pre = Preprocessor()\n",
    "pre.cast(dtype=tf.float32).downscale(max_pixel_count=512**2).pad(network_levels=levels)\n",
    "\n",
    "prefull = Preprocessor()\n",
    "prefull.cast(dtype=tf.float32).normalize().downscale(max_pixel_count=512**2).pad(network_levels=levels)\n",
    "\n",
    "scheduler = WorkScheduler()\n",
    "scheduler.add_data(None, save_x, dataset_dir = dataset_dir, out_dir = x_dir, preprocessor = pre)\n",
    "scheduler.add_data(None, save_y, dataset_dir = dataset_dir, out_dir = y_dir,  preprocessor = prefull)\n",
    "scheduler.do_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY1CtzlnKoMb"
   },
   "outputs": [],
   "source": [
    "# load files to drive as ZIP or else it will be really slow to reload them into colab\n",
    "\n",
    "zip_path = 'drive/MyDrive/testing/skinny/' + timestr + '_xy.zip'\n",
    "\n",
    "# zip\n",
    "!zip -r $zip_path $timestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_gBskrW18l5"
   },
   "source": [
    "-- OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rq1YAAUYFTYP"
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset_dir=r'dataset', batch_size=1, preprocessor=preprocessor)\n",
    "tensor_dataset = create_dataset_pipeline(self, 'images', shuffle = False)\n",
    "\n",
    "# model.predict(tensor_dataset) # and save them to a folder, maybe see benchmark notebook methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gLfKpkOLGF5"
   },
   "outputs": [],
   "source": [
    "# load saved models\n",
    "\n",
    "!mkdir -p logs/Skinny/checkpoint/saved_model.ckpt\n",
    "!cp -r drive/MyDrive/training/skinny/checkpoint-/20210411-225022/saved_model.ckpt/. logs/Skinny/checkpoint/saved_model.ckpt\n",
    "\n",
    "!mkdir -p logs/Skinny2/checkpoint\n",
    "!cp -r drive/MyDrive/training/skinny/. logs/Skinny2/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNGCFhihKUdM"
   },
   "outputs": [],
   "source": [
    "# directory\n",
    "# eg [ckpt]: logs/Skinny/checkpoint/saved_model.ckpt\n",
    "#            (saved_model.ckpt is neither a file nor a folder, but in the\n",
    "#             checkpoint folders there are 3 files:\n",
    "#             checkpoint, saved_model.ckpt.index, saved_model.ckpt.data-00000-of-00001)\n",
    "# eg [SavedModel]: logs/Skinny/checkpoint/saved_model.ckpt\n",
    "#                  (saved_model.ckpt is a folder and contains saved_model.pb)\n",
    "def my_load_model(directory, load_weights: bool = True):\n",
    "    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True, model_name=model_name)\n",
    "    mod.get_model()\n",
    "\n",
    "    if load_weights:\n",
    "        path = os.path.join(directory)\n",
    "        mod.keras_model.load_weights(path)\n",
    "    #else:\n",
    "    #    mod = Skinny(levels, initial_filters, image_channels, log_dir, load_checkpoint=True, model_name=model_name)\n",
    "    \n",
    "    #mod.get_model()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "S7_LSJLRKdK6",
    "outputId": "6235980f-348c-4f62-a98c-aaf5222c03ba"
   },
   "outputs": [],
   "source": [
    "# path = 'dataset/org/features/im00002.jpg'\n",
    "def get_prediction(path, preprocessor, model):\n",
    "    ds = my_loader(path, preprocessor)\n",
    "    ds = my_batch(ds, 1)\n",
    "    \n",
    "    trainer = Trainer(data_loader=ds, model=model, evaluate_test_data=True)\n",
    "    trainer.add_losses([K.binary_crossentropy, dice_loss])\n",
    "    trainer.add_metrics([\n",
    "        f1,\n",
    "        iou,\n",
    "        precision,\n",
    "        recall\n",
    "    ])\n",
    "\n",
    "    model.get_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    model.keras_model.compile(optimizer=optimizer, loss=trainer.combined_loss(), metrics=trainer.metrics)\n",
    "\n",
    "    for string_, int_ in ds:\n",
    "        img = string_\n",
    "        break\n",
    "    \n",
    "    arr = np.array([img['feature'].numpy()[0]])\n",
    "    prediction = model.keras_model.predict(arr)\n",
    "    return prediction[0]*255\n",
    "\n",
    "#prediction = get_prediction('dataset/org/features/im00003.jpg', preprocessor, my_load_model('logs/Skinny/checkpoint/saved_model.ckpt', load_weights=False))\n",
    "prediction = get_prediction('dataset/org/features/im00043.jpg', preprocessor, my_load_model('logs/Skinny2/checkpoint/saved_model.ckpt'))\n",
    "cv2_imshow(prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "general_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
